try:
    import pysqlite3
    import sys
    sys.modules["sqlite3"] = sys.modules.pop("pysqlite3")
except ImportError:
    pass


from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings   
from langchain_chroma import Chroma
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain_openai import ChatOpenAI
from langchain import hub
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
import streamlit as st
import tempfile
import os
from dotenv import load_dotenv
load_dotenv()


#ì œëª©
st.title("ChatPDF with rag")
st.write("-------------------------")

# íŒŒì¼ì—…ë¡œë“œ
uploaded_file = st.file_uploader("PDF íŒŒì¼ì„ ì˜¬ë ¤ì£¼ì„¸ìš”", type=['pdf'])
st.write("------------")


def pdf_to_document(uploaded_file):
    temp_dir = tempfile.TemporaryDirectory()
    temp_filepath= os.path.join(temp_dir.name, uploaded_file.name)
    with open(temp_filepath,"wb") as f:
        f.write(uploaded_file.getvalue())
    loader = PyPDFLoader(temp_filepath)
    pages=loader.load_and_split()
    return pages

#ì—…ë¡œë“œëœ íŒŒì¼ì²˜ë¦¬

if uploaded_file is not None:
    st.success(f"âœ… {uploaded_file.name} ì—…ë¡œë“œ ì™„ë£Œ")
    pages=pdf_to_document(uploaded_file)
    st.write(f"ğŸ“‘ ì´ {len(pages)} í˜ì´ì§€ ë¡œë“œë¨")



    #loader = PyPDFLoader("unsu.pdf")
    #pages = loader.load_and_split()

    #Splitter
    text_splitter = RecursiveCharacterTextSplitter(
        # Set a really small chunk size, just to show.
        chunk_size=300,
        chunk_overlap = 20,
        length_function = len,
        is_separator_regex = False,
    
    )
    if pages:
        texts = text_splitter.split_documents(pages)



        #embeddings
        embeddings_model = OpenAIEmbeddings(model="text-embedding-3-small",
            # With the 'text-embedding-3' class
            # if models, you can specify the size
            # if the embeddings you wnat returned
            # dimensions=1024
        )

        import chromadb
        chromadb.api.client.SharedSystemClient.clear_system_cache()

        #chroma DB
        db= Chroma.from_documents(texts, embeddings_model)
    else:
        st.error("âŒ PDFì—ì„œ í˜ì´ì§€ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
else:
    st.warning("ğŸ“‚ ë¨¼ì € PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”")
#User Input
st.header("PDFì—ê²Œ ì§ˆë¬¸í•´ë³´ì„¸ìš”!!")
question=st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")

if st.button("ì§ˆë¬¸í•˜ê¸°"):
    with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
        #Retriver
        # question = "ì•„ë‚´ê°€ ë¨¹ê³ ì‹¶ì–´í•˜ëŠ” ìŒì‹ì´ ë­ì•¼?"
        llm = ChatOpenAI(temperature=0)
        retriever_from_llm = MultiQueryRetriever.from_llm(
            retriever=db.as_retriever(), llm=llm
        )
    

        # prompt Template
        prompt = hub.pull("rlm/rag-prompt")

        # docs = retriever_from_llm.invoke(question)
        # print(len(docs))
        # print(docs)

        # Generate
        def format_docs(docs):
            return "\n\n".join([doc.page_content for doc in docs])
        rag_chain = (
            {"context": retriever_from_llm | format_docs, "question" : RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
        )

        #Question
        result = rag_chain.invoke(question)

        st.write(result)


